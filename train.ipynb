{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from load_glove import Embedding, Glove, LoadPretrainedGlove\n",
    "from models import SkipGram, CBOW, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoadPretrainedGlove was implemented to load the 27B pre-trained Glove embeddings\n",
    "# the default was set to use the 100d embedding model\n",
    "# can pass in a filepath for other embedding dimensions to load other pre-trained embeddings\n",
    "# Glove 27B has 25d, 50d, 100d, and 200d online. Note that 200d takes a long time to load (27 billions * 200 total floats)\n",
    "# LoadPretrainedGlove returns a Glove object\n",
    "EMBEDDING_DIM = 100\n",
    "glove_model = LoadPretrainedGlove() # use default 100d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Michael' not in the model\n"
     ]
    }
   ],
   "source": [
    "# test functionality of the model (can skip), so far I only implemented five methods\n",
    "# get_vector will get the embeddings of a token (Glove made all tokens lower cases)\n",
    "glove_model.get_vector('Michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7804909766934288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functionality of the model (can skip), so far I only implemented five methods\n",
    "# similarity will retrieve the embeddings and compute the consine similarity between two given tokens\n",
    "glove_model.similarity('excellent', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.7891919692639917),\n",
       " ('great', 0.7804909766934288),\n",
       " ('brilliant', 0.7707372695455804),\n",
       " ('superb', 0.7450593638693023),\n",
       " ('outstanding', 0.7220487414581103),\n",
       " ('terrific', 0.7189632766496982),\n",
       " ('unique', 0.713248325184925),\n",
       " ('article', 0.7111885947728657),\n",
       " ('very', 0.7083041440709903),\n",
       " ('interesting', 0.7079968065670241)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functionality of the model (can skip), so far I only implemented five methods\n",
    "# most_similar can be used to map the word embedding back to the most similar word in the model using cosine\n",
    "glove_model.most_similar('excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functionality of the model (can skip), so far I only implemented five methods\n",
    "# similarity_embedding can be used to compute the similarity between two input embeddings\n",
    "# it basically just compute the consine similarity between two vectors\n",
    "glove_model.similarity_embedding([1, 0, 1], [1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.7891919692639917),\n",
       " ('great', 0.7804909766934288),\n",
       " ('brilliant', 0.7707372695455804),\n",
       " ('superb', 0.7450593638693023),\n",
       " ('outstanding', 0.7220487414581103),\n",
       " ('terrific', 0.7189632766496982),\n",
       " ('unique', 0.713248325184925),\n",
       " ('article', 0.7111885947728657),\n",
       " ('very', 0.7083041440709903),\n",
       " ('interesting', 0.7079968065670241)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functionality of the model (can skip), so far I only implemented five methods\n",
    "# most_similar_token find the most similar words with the input word embedding\n",
    "# by default it returns top 10 most similar words and the similarity with the input embedding\n",
    "# can change top_n to any number\n",
    "glove_model.most_similar_token(glove_model.get_vector('excellent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'...' not in the model\n",
      "'16.12.00' not in the model\n",
      "'all-events' not in the model\n",
      "'16th' not in the model\n",
      "'2000' not in the model\n",
      "'-lrb-' not in the model\n",
      "'-rrb-' not in the model\n",
      "'s/he' not in the model\n",
      "'24' not in the model\n",
      "'1' not in the model\n",
      "'2' not in the model\n",
      "'3' not in the model\n",
      "'just-so-called' not in the model\n",
      "'11' not in the model\n",
      "'characterizes' not in the model\n",
      "'ôinternational' not in the model\n",
      "'festivalö' not in the model\n",
      "'ôseveralö' not in the model\n",
      "'sandwich-man' not in the model\n",
      "'ôsmallö' not in the model\n",
      "'ôbest' not in the model\n",
      "'ideaö' not in the model\n",
      "'ôsuch' not in the model\n",
      "'lifeà' not in the model\n",
      "'21' not in the model\n",
      "'22' not in the model\n",
      "'wezy' not in the model\n",
      "'15' not in the model\n",
      "'6' not in the model\n",
      "'100' not in the model\n",
      "'fourthly' not in the model\n",
      "'5' not in the model\n",
      "'16' not in the model\n",
      "'pony-ride' not in the model\n",
      "'21-22' not in the model\n",
      "'21st-22nd' not in the model\n",
      "'quarrelled' not in the model\n",
      "'!!' not in the model\n",
      "'3,000' not in the model\n",
      "'fewà' not in the model\n",
      "'16/12/2000' not in the model\n",
      "'22nd' not in the model\n",
      "'9' not in the model\n",
      "'no-smoking' not in the model\n",
      "'8' not in the model\n",
      "'computer-user' not in the model\n",
      "'well-organised' not in the model\n",
      "'ôdo' not in the model\n",
      "'ten-minute' not in the model\n",
      "'10.30' not in the model\n",
      "'brillient' not in the model\n",
      "'nolbu' not in the model\n",
      "'hungbu' not in the model\n",
      "'interestedin' not in the model\n",
      "'weeker' not in the model\n",
      "'uniform-like' not in the model\n",
      "'moldava' not in the model\n",
      "'smetana' not in the model\n",
      "'sir/madam' not in the model\n",
      "'19' not in the model\n",
      "'18' not in the model\n",
      "'25th' not in the model\n",
      "'10th' not in the model\n",
      "'10' not in the model\n",
      "'pocket-money' not in the model\n",
      "'1000' not in the model\n",
      "'21st' not in the model\n",
      "'u2' not in the model\n",
      "'16.12.2000' not in the model\n",
      "'alternations' not in the model\n",
      "'bizen' not in the model\n",
      "'onigashima' not in the model\n",
      "'kibidango' not in the model\n",
      "'stepparents' not in the model\n",
      "'aaroun' not in the model\n",
      "'tazief' not in the model\n",
      "'17th' not in the model\n",
      "'19.30' not in the model\n",
      "'20.15' not in the model\n",
      "'intransigent' not in the model\n",
      "'23.45' not in the model\n",
      "'20' not in the model\n",
      "'40' not in the model\n",
      "'19:30' not in the model\n",
      "'20:15' not in the model\n",
      "'pratitable' not in the model\n",
      "'45' not in the model\n",
      "'59' not in the model\n",
      "'al1' not in the model\n",
      "'2hw' not in the model\n",
      "'15th' not in the model\n",
      "'tyung' not in the model\n",
      "'sargo' not in the model\n",
      "'co2' not in the model\n",
      "'17.06.2000' not in the model\n",
      "'12th' not in the model\n",
      "'19.00' not in the model\n",
      "'hedwiga' not in the model\n",
      "'heinkel' not in the model\n",
      "'safety-money' not in the model\n",
      "'atrophided' not in the model\n",
      "'200' not in the model\n",
      "'17' not in the model\n",
      "'2100' not in the model\n",
      "'14:30' not in the model\n",
      "'volencia' not in the model\n",
      "'vardez' not in the model\n",
      "'bay-side' not in the model\n",
      "'250' not in the model\n",
      "'14.30' not in the model\n",
      "'t.vs' not in the model\n",
      "'1960' not in the model\n",
      "'belabara' not in the model\n",
      "'calzari' not in the model\n",
      "'mini-skirts' not in the model\n",
      "'20th' not in the model\n",
      "'3rd' not in the model\n",
      "'!!!!' not in the model\n",
      "'neubathin' not in the model\n",
      "'sky-train' not in the model\n",
      "'air-conditioner' not in the model\n",
      "'peissaud' not in the model\n",
      "'8.15' not in the model\n",
      "'9.30' not in the model\n",
      "'redecoration' not in the model\n",
      "'50' not in the model\n",
      "'skoclow' not in the model\n",
      "'43-430' not in the model\n",
      "'17/06/00' not in the model\n",
      "'camirez' not in the model\n",
      "'19.20' not in the model\n",
      "'comfortcomfortable' not in the model\n",
      "'full-price' not in the model\n",
      "'17-6-00' not in the model\n",
      "'1999' not in the model\n",
      "'rapidity' not in the model\n",
      "'1914' not in the model\n",
      "'30' not in the model\n",
      "'9th' not in the model\n",
      "'1996' not in the model\n",
      "'scandal-mongering' not in the model\n",
      "'taeron' not in the model\n",
      "'20.00' not in the model\n",
      "'cyber-looking' not in the model\n",
      "'fish-liked-suit' not in the model\n",
      "'18:00' not in the model\n",
      "'narators' not in the model\n",
      "'tutry' not in the model\n",
      "'19:00' not in the model\n",
      "'concretely' not in the model\n",
      "'santoquez' not in the model\n",
      "'frenetically' not in the model\n",
      "'resonse' not in the model\n",
      "'500' not in the model\n",
      "'alarm-clock' not in the model\n",
      "'2-3' not in the model\n",
      "'20:30' not in the model\n",
      "'25-year-old' not in the model\n",
      "'timson' not in the model\n",
      "'madam/sir' not in the model\n",
      "'above-mentioned' not in the model\n",
      "'hand-write' not in the model\n",
      "'kresnodar' not in the model\n",
      "'cell-phones' not in the model\n",
      "'attented' not in the model\n",
      "'17/6/00' not in the model\n",
      "'2100s' not in the model\n",
      "'pasimov' not in the model\n",
      "'sanou' not in the model\n",
      "'sun-young' not in the model\n",
      "'jarova' not in the model\n",
      "'e.h.' not in the model\n",
      "'chitchee' not in the model\n",
      "'1989' not in the model\n",
      "'arm-suits' not in the model\n",
      "'6th' not in the model\n",
      "'12' not in the model\n",
      "'08532143251' not in the model\n",
      "'schwitt' not in the model\n",
      "'rucented' not in the model\n",
      "'canidates' not in the model\n",
      "'8:15' not in the model\n",
      "'160' not in the model\n",
      "'hähnel' not in the model\n",
      "'104' not in the model\n",
      "'mühlenstrasse' not in the model\n",
      "'09111' not in the model\n",
      "'chemnik' not in the model\n",
      "'14th' not in the model\n",
      "'30s' not in the model\n",
      "'00.15' not in the model\n",
      "'00.00' not in the model\n",
      "'he/she' not in the model\n",
      "'17.6.2000' not in the model\n",
      "'almodar' not in the model\n",
      "'19,30' not in the model\n",
      "'60' not in the model\n",
      "'50-25' not in the model\n",
      "'8:00' not in the model\n",
      "'painting-works' not in the model\n",
      "'beschle' not in the model\n",
      "'bo-phat' not in the model\n",
      "'acid-rain' not in the model\n",
      "'conciderly' not in the model\n",
      "'7:30' not in the model\n",
      "'41-minute' not in the model\n",
      "'dinning-hall' not in the model\n",
      "'posthaste' not in the model\n",
      "'sigone' not in the model\n",
      "'beroni' not in the model\n",
      "'atrophied' not in the model\n",
      "'000' not in the model\n",
      "'computer-aided' not in the model\n",
      "'furilo' not in the model\n",
      "'5th' not in the model\n",
      "'uv-rays' not in the model\n",
      "'fansisco' not in the model\n",
      "'roddos' not in the model\n",
      "'19:20' not in the model\n",
      "'kitchen-device' not in the model\n",
      "'high-technology' not in the model\n",
      "'sun-glasses' not in the model\n",
      "'suriphat' not in the model\n",
      "'21:00' not in the model\n",
      "'23456789' not in the model\n",
      "'chuaswee' not in the model\n",
      "'1st' not in the model\n",
      "'omadar' not in the model\n",
      "'peculiarly' not in the model\n",
      "'canalized' not in the model\n",
      "'fomalar' not in the model\n",
      "'19:15' not in the model\n",
      "'55' not in the model\n",
      "'18.30' not in the model\n",
      "'untraditional' not in the model\n",
      "'16-year-old' not in the model\n",
      "'6-year-old' not in the model\n",
      "'musical-show' not in the model\n",
      "'ultra-violet-proof' not in the model\n",
      "'dangerous-liquid-proof' not in the model\n",
      "'great-grandparents' not in the model\n",
      "'washing-up' not in the model\n",
      "'reitgard' not in the model\n",
      "'p1' not in the model\n",
      "'86' not in the model\n",
      "'animal-pattern' not in the model\n",
      "'25' not in the model\n",
      "'gregore' not in the model\n",
      "'7.30' not in the model\n",
      "'7' not in the model\n",
      "'video-recorders' not in the model\n",
      "'marvellously' not in the model\n",
      "'menbrook' not in the model\n",
      "'raulet' not in the model\n",
      "'school-mates' not in the model\n",
      "'18th' not in the model\n",
      "'18:30' not in the model\n",
      "'data-bank' not in the model\n",
      "'half-past' not in the model\n",
      "'90s' not in the model\n",
      "'486' not in the model\n",
      "'90' not in the model\n",
      "'persuasively' not in the model\n",
      "'fifty-two' not in the model\n",
      "'fanetti' not in the model\n",
      "'sersuwan' not in the model\n",
      "'chlororganics' not in the model\n",
      "'degased' not in the model\n",
      "'gaussier' not in the model\n",
      "'16/6' not in the model\n",
      "'svenssen' not in the model\n",
      "'techonology' not in the model\n",
      "'enjure' not in the model\n",
      "'??' not in the model\n",
      "'numertable' not in the model\n",
      "'ledia' not in the model\n",
      "'loredes' not in the model\n",
      "'tranbles' not in the model\n",
      "'23rd' not in the model\n",
      "'schulsson' not in the model\n",
      "'17-06-00' not in the model\n",
      "'50.00' not in the model\n",
      "'phukana' not in the model\n",
      "'7:00' not in the model\n",
      "'comatova' not in the model\n",
      "'future-clothing' not in the model\n",
      "'tv-set' not in the model\n",
      "'elise-lune' not in the model\n",
      "'48' not in the model\n",
      "'1093' not in the model\n",
      "'2:30' not in the model\n",
      "'excisted' not in the model\n",
      "'diappointing' not in the model\n",
      "'bergut' not in the model\n",
      "'hunrtner' not in the model\n",
      "'70' not in the model\n",
      "'caesara' not in the model\n",
      "'rodrigos' not in the model\n",
      "'arnos' not in the model\n",
      "'key-word' not in the model\n",
      "'show.' not in the model\n",
      "'?!' not in the model\n",
      "'4' not in the model\n",
      "'sapawadee' not in the model\n",
      "'prisuntha' not in the model\n",
      "'f.o.f.' not in the model\n",
      "'2nd' not in the model\n",
      "'3f' not in the model\n",
      "'!?' not in the model\n",
      "'577-6277' not in the model\n",
      "'dish-washer' not in the model\n",
      "'monographies' not in the model\n",
      "'21.00' not in the model\n",
      "'!!!' not in the model\n",
      "'?!!' not in the model\n",
      "'9:15' not in the model\n",
      "'9:30' not in the model\n",
      "'busybodies' not in the model\n",
      "'telefaxes' not in the model\n",
      "'robotcap' not in the model\n",
      "'rumgopol' not in the model\n",
      "'rinsook' not in the model\n",
      "'1900s' not in the model\n",
      "'ongelina' not in the model\n",
      "'8.00' not in the model\n",
      "'estatement' not in the model\n",
      "'45-minute' not in the model\n",
      "'suntavee' not in the model\n",
      "'20:75' not in the model\n",
      "'abysmally' not in the model\n",
      "'enitieising' not in the model\n",
      "'favourably' not in the model\n",
      "'phittikhun' not in the model\n",
      "'patharius' not in the model\n",
      "'porawek' not in the model\n",
      "'phowrung' not in the model\n",
      "'smilingly' not in the model\n",
      "'1973' not in the model\n",
      "'grately' not in the model\n",
      "'11:00' not in the model\n",
      "'11:30' not in the model\n",
      "'ounis' not in the model\n",
      "'22:00' not in the model\n",
      "'eingrid' not in the model\n",
      "'yakimo' not in the model\n",
      "'nashoto' not in the model\n",
      "'22.00' not in the model\n",
      "'phatosub' not in the model\n",
      "'raverno' not in the model\n",
      "'jin-sun' not in the model\n",
      "'techology' not in the model\n",
      "'sweet-looking' not in the model\n",
      "'reasently' not in the model\n",
      "'emsamblish' not in the model\n",
      "'attencionally' not in the model\n",
      "'straussburg' not in the model\n",
      "'mourk' not in the model\n",
      "'inversed' not in the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'23456' not in the model\n",
      "'17/6/2000' not in the model\n",
      "'dodgy-looking' not in the model\n",
      "'big-mouth' not in the model\n",
      "'pouded' not in the model\n",
      "'barodo' not in the model\n",
      "'actor/actress' not in the model\n",
      "'mobile-phone' not in the model\n",
      "'19,00' not in the model\n",
      "'vaisili' not in the model\n",
      "'solatov' not in the model\n",
      "'17/06/2000' not in the model\n",
      "'2.00' not in the model\n",
      "'2.30' not in the model\n",
      "'19.15' not in the model\n",
      "'ann1' not in the model\n",
      "'19:25' not in the model\n",
      "'petrol-powered' not in the model\n",
      "'helgar' not in the model\n",
      "'svessen' not in the model\n",
      "'1874' not in the model\n",
      "'57' not in the model\n",
      "'gigantically' not in the model\n",
      "'1986' not in the model\n",
      "'1988' not in the model\n",
      "'unforgivably' not in the model\n",
      "'34' not in the model\n",
      "'kingfield' not in the model\n",
      "'gc2' not in the model\n",
      "'8as' not in the model\n",
      "'1/6/00' not in the model\n",
      "'otherstuff' not in the model\n",
      "'kaczmarski' not in the model\n",
      "'13.06.2000' not in the model\n",
      "'bwpm' not in the model\n",
      "'information-technology' not in the model\n",
      "'netserver' not in the model\n",
      "'13.06.00' not in the model\n",
      "'mararcanã' not in the model\n",
      "'23' not in the model\n",
      "'clutone' not in the model\n",
      "'thirtieth' not in the model\n",
      "'14' not in the model\n",
      "'13th' not in the model\n",
      "'x300' not in the model\n",
      "'nvq3' not in the model\n",
      "'and/or' not in the model\n",
      "'becamehave' not in the model\n",
      "'1998' not in the model\n",
      "'13/06/00' not in the model\n",
      "'next-term' not in the model\n",
      "'350' not in the model\n",
      "'moicznos' not in the model\n",
      "'pantelis' not in the model\n",
      "'13' not in the model\n",
      "'6:30' not in the model\n",
      "'8:30' not in the model\n",
      "'14.06.2000' not in the model\n",
      "'mcpartney' not in the model\n",
      "'i.s.o.' not in the model\n",
      "'preocupation' not in the model\n",
      "'pre-prepared' not in the model\n",
      "'15-5' not in the model\n",
      "'titov' not in the model\n",
      "'singapoor' not in the model\n",
      "'mrs/miss' not in the model\n",
      "'gayarre' not in the model\n",
      "'bar-tending' not in the model\n",
      "'mile-long' not in the model\n",
      "'125th' not in the model\n",
      "'e-shopping' not in the model\n",
      "'paner' not in the model\n",
      "'7.00' not in the model\n",
      "'00' not in the model\n",
      "'free-time' not in the model\n",
      "'30th' not in the model\n",
      "'pop-concert' not in the model\n",
      "'middle-high' not in the model\n",
      "'daily-life' not in the model\n",
      "'bonfilia' not in the model\n",
      "'well-preferred' not in the model\n",
      "'realisable' not in the model\n",
      "'4:30' not in the model\n",
      "'polish-origin' not in the model\n",
      "'demoriane' not in the model\n",
      "'outlights' not in the model\n",
      "'school-wide' not in the model\n",
      "'guillelmina' not in the model\n",
      "'pop-stars' not in the model\n",
      "'13/6/2000' not in the model\n",
      "'-11-07' not in the model\n",
      "'coblights' not in the model\n",
      "'10/05/2000' not in the model\n",
      "'adalbert' not in the model\n",
      "'turnau' not in the model\n",
      "'100,000' not in the model\n",
      "'chmielewski' not in the model\n",
      "'harrodss' not in the model\n",
      "'duteil' not in the model\n",
      "'oliviar' not in the model\n",
      "'3-week' not in the model\n",
      "'experimentally' not in the model\n",
      "'water-polo' not in the model\n",
      "'delerm' not in the model\n",
      "'heanly' not in the model\n",
      "'heanley' not in the model\n",
      "'lay-by' not in the model\n",
      "'szymanski' not in the model\n",
      "'dumaux' not in the model\n",
      "'27th' not in the model\n",
      "'spectaculars' not in the model\n",
      "'school-team' not in the model\n",
      "'xzy' not in the model\n",
      "'vanida' not in the model\n",
      "'montri' not in the model\n",
      "'1th' not in the model\n",
      "'28th' not in the model\n",
      "'ae-sook' not in the model\n",
      "'intermidiate' not in the model\n",
      "'yearsfor' not in the model\n",
      "'13/6/00' not in the model\n",
      "'first-prize' not in the model\n",
      "'volavkova' not in the model\n",
      "'80' not in the model\n",
      "'gerian' not in the model\n",
      "'28' not in the model\n",
      "'two-weeks' not in the model\n",
      "'native-speaker' not in the model\n",
      "'keenness' not in the model\n",
      "'top-pop' not in the model\n",
      "'gapon' not in the model\n",
      "'gubin' not in the model\n",
      "'kozin' not in the model\n",
      "'800000000' not in the model\n",
      "'tanizaki' not in the model\n",
      "'tuesdy' not in the model\n",
      "'f.c.e.' not in the model\n",
      "'duangkamol' not in the model\n",
      "'saowaluk' not in the model\n",
      "'nineteen-year-old' not in the model\n",
      "'fu-su' not in the model\n",
      "'grebenchekoff' not in the model\n",
      "'10/2000' not in the model\n",
      "'needi' not in the model\n",
      "'shop-assistant' not in the model\n",
      "'36' not in the model\n",
      "'eye-shadow' not in the model\n",
      "'38' not in the model\n",
      "'haribands' not in the model\n",
      "'lipnicka' not in the model\n",
      "'stenkovska' not in the model\n",
      "'24th' not in the model\n",
      "'u.sa' not in the model\n",
      "'85' not in the model\n",
      "'repellants' not in the model\n",
      "'thanapol' not in the model\n",
      "'a1' not in the model\n",
      "'87022' not in the model\n",
      "'24-hour' not in the model\n",
      "'symbolical' not in the model\n",
      "'amelin' not in the model\n",
      "'new-arrivals' not in the model\n",
      "'ratana' not in the model\n",
      "'ikurapiura' not in the model\n",
      "'after-show' not in the model\n",
      "'warshaw' not in the model\n",
      "'scenography' not in the model\n",
      "'rulled' not in the model\n",
      "'mrs/ms' not in the model\n",
      "'2-hour' not in the model\n",
      "'herberto' not in the model\n",
      "'d'accatino' not in the model\n",
      "'kazanova' not in the model\n",
      "'department-store' not in the model\n",
      "'1980' not in the model\n",
      "'musicanyway' not in the model\n",
      "'liaise' not in the model\n",
      "'?!!!' not in the model\n",
      "'2001' not in the model\n",
      "'cranberies' not in the model\n",
      "'13.6.00' not in the model\n",
      "'jimme' not in the model\n",
      "'rachiat' not in the model\n",
      "'thongchai' not in the model\n",
      "'enpheno' not in the model\n",
      "'2:00' not in the model\n",
      "'15m' not in the model\n",
      "'back-to-basic' not in the model\n",
      "'venerio' not in the model\n",
      "'sound-check' not in the model\n",
      "'barcicka' not in the model\n",
      "'45-139' not in the model\n",
      "'piotrowska' not in the model\n",
      "'arniston' not in the model\n",
      "'largish' not in the model\n",
      "'meong-dong' not in the model\n",
      "'sunee' not in the model\n",
      "'boonliang-hampson' not in the model\n",
      "'zacharie' not in the model\n",
      "'albito' not in the model\n",
      "'1997' not in the model\n",
      "'agripina' not in the model\n",
      "'tadeusz' not in the model\n",
      "'ochman' not in the model\n",
      "'13-06-00' not in the model\n",
      "'pabori' not in the model\n",
      "'hypermarkets' not in the model\n",
      "'cholmondeley' not in the model\n",
      "'rosalinde' not in the model\n",
      "'7th' not in the model\n",
      "'deaps' not in the model\n",
      "'self-catering' not in the model\n",
      "'13rd' not in the model\n",
      "'105' not in the model\n",
      "'fukao' not in the model\n",
      "'as3' not in the model\n",
      "'7gh' not in the model\n",
      "'sarted' not in the model\n",
      "'6:00' not in the model\n",
      "'12:00' not in the model\n",
      "'50,000' not in the model\n",
      "'meesee' not in the model\n",
      "'staylist' not in the model\n",
      "'candoso' not in the model\n",
      "'eating-place' not in the model\n",
      "'29th' not in the model\n",
      "'mee-yon' not in the model\n",
      "'surganova' not in the model\n",
      "'golf-courses' not in the model\n",
      "'bornete' not in the model\n",
      "'b.h.' not in the model\n",
      "'pot-au-fleurs' not in the model\n",
      "'demuro' not in the model\n",
      "'ecstasies' not in the model\n",
      "'zalai' not in the model\n",
      "'steat' not in the model\n",
      "'shopmania' not in the model\n",
      "'27.66' not in the model\n",
      "'discotheques' not in the model\n",
      "'longbut' not in the model\n",
      "'gize' not in the model\n",
      "'kingsberry' not in the model\n",
      "'cs4' not in the model\n",
      "'1xd' not in the model\n",
      "'well-equipped' not in the model\n",
      "'up3' not in the model\n",
      "'tramits' not in the model\n",
      "'31' not in the model\n",
      "'long-looked-forward-to' not in the model\n",
      "'11th' not in the model\n",
      "'jagoares' not in the model\n",
      "'hernañdez' not in the model\n",
      "'boonsri' not in the model\n",
      "'thanasukolwit' not in the model\n",
      "'thisbecause' not in the model\n",
      "'finebecause' not in the model\n",
      "'mebecause' not in the model\n",
      "'1993' not in the model\n",
      "'long-faced' not in the model\n",
      "'13/june/2000' not in the model\n",
      "'amidio' not in the model\n",
      "'ryam' not in the model\n",
      "'poolo' not in the model\n",
      "'bovie' not in the model\n",
      "'conections' not in the model\n",
      "'penfriend' not in the model\n",
      "'svaves' not in the model\n",
      "'actuation' not in the model\n",
      "'uningative' not in the model\n",
      "'26' not in the model\n",
      "'13nd' not in the model\n",
      "'2,000' not in the model\n",
      "'usedmoreover' not in the model\n",
      "'31st' not in the model\n",
      "'nowthere' not in the model\n",
      "'5.30' not in the model\n",
      "'saleswomen' not in the model\n",
      "'miss/mrs' not in the model\n",
      "'franscico' not in the model\n",
      "'1200' not in the model\n",
      "'400' not in the model\n",
      "'barwoman' not in the model\n",
      "'sobczak' not in the model\n",
      "'night-club' not in the model\n",
      "'edith-marilyn' not in the model\n",
      "'single-handicapped' not in the model\n",
      "'5,000' not in the model\n",
      "'20:00' not in the model\n",
      "'20:45' not in the model\n",
      "'almendegos' not in the model\n",
      "'8430' not in the model\n",
      "'chedicard' not in the model\n",
      "'sintawichai' not in the model\n",
      "'chaow' not in the model\n",
      "'silver-coloured' not in the model\n",
      "'13/06/2000' not in the model\n",
      "'encloses' not in the model\n",
      "'brönte' not in the model\n",
      "'yeun-ja' not in the model\n",
      "'300' not in the model\n",
      "'18-28' not in the model\n",
      "'especiallyabout' not in the model\n",
      "'vasiliy' not in the model\n",
      "'alekseyev' not in the model\n",
      "'120' not in the model\n",
      "'prasert' not in the model\n",
      "'charoenkul' not in the model\n",
      "'rattanakosin' not in the model\n",
      "'4,000' not in the model\n",
      "'4th' not in the model\n",
      "'ferndown' not in the model\n",
      "'10.00' not in the model\n",
      "'reorganise' not in the model\n",
      "'implove' not in the model\n",
      "'sports-wear' not in the model\n",
      "'12.12.2000' not in the model\n",
      "'10:00' not in the model\n",
      "'3-day' not in the model\n",
      "'unhabitual' not in the model\n",
      "'his/her' not in the model\n",
      "'scientic' not in the model\n",
      "'5000' not in the model\n",
      "'n3c' not in the model\n",
      "'oxana' not in the model\n",
      "'irkind' not in the model\n",
      "'13.00' not in the model\n",
      "'non-breakable' not in the model\n",
      "'house-keeping' not in the model\n",
      "'10.00-19.00' not in the model\n",
      "'olcoy' not in the model\n",
      "'12.12.0' not in the model\n",
      "'12.12.00' not in the model\n",
      "'10:00-19:00' not in the model\n",
      "'cryptographic' not in the model\n",
      "'3cd' not in the model\n",
      "'filmstars' not in the model\n",
      "'eirini' not in the model\n",
      "'20000ft' not in the model\n",
      "'fast-developing' not in the model\n",
      "'tv-sets' not in the model\n",
      "'wawsaw' not in the model\n",
      "'1/3/2000' not in the model\n",
      "'13th-14th' not in the model\n",
      "'twenty-metre' not in the model\n",
      "'15.00' not in the model\n",
      "'1600' not in the model\n",
      "'1900' not in the model\n",
      "'mass-media' not in the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bao-yu' not in the model\n",
      "'kang-nam' not in the model\n",
      "'disturbers' not in the model\n",
      "'mid-forties' not in the model\n",
      "'eagle-eyed' not in the model\n",
      "'well-proven' not in the model\n",
      "'co-operates' not in the model\n",
      "'decline-line' not in the model\n",
      "'32' not in the model\n",
      "'3,700-metre-high' not in the model\n",
      "'arolla' not in the model\n",
      "'1995' not in the model\n",
      "'steepness' not in the model\n",
      "'full-bloomed' not in the model\n",
      "'tiptoed' not in the model\n",
      "'km/hr' not in the model\n",
      "'voice-controlled' not in the model\n",
      "'march.' not in the model\n",
      "'somary' not in the model\n",
      "'3c' not in the model\n",
      "'12-12-00' not in the model\n",
      "'29' not in the model\n",
      "'4a' not in the model\n",
      "'2113' not in the model\n",
      "'3-6' not in the model\n",
      "'3:00' not in the model\n",
      "'6.30' not in the model\n",
      "'twenty-fourth' not in the model\n",
      "'carleon' not in the model\n",
      "'end-of-conference' not in the model\n",
      "'chêne-bourg' not in the model\n",
      "'well-situated' not in the model\n",
      "'bodnant' not in the model\n",
      "'motorization' not in the model\n",
      "'35' not in the model\n",
      "'nyremberg' not in the model\n",
      "'13:00' not in the model\n",
      "'13:30' not in the model\n",
      "'mini-bus' not in the model\n",
      "'semi-classical' not in the model\n",
      "'1975' not in the model\n",
      "'stock-market' not in the model\n",
      "'3-d' not in the model\n",
      "'83' not in the model\n",
      "'5:30' not in the model\n",
      "'9:00' not in the model\n",
      "'200000' not in the model\n",
      "'500,000' not in the model\n",
      "'hernon' not in the model\n",
      "'234' not in the model\n",
      "'kensigton' not in the model\n",
      "'0033' not in the model\n",
      "'1638' not in the model\n",
      "'8:45' not in the model\n",
      "'fire-fighters' not in the model\n",
      "'johanesburg' not in the model\n",
      "'yverdon' not in the model\n",
      "'xviii' not in the model\n",
      "'satisfactorily' not in the model\n",
      "'romanic' not in the model\n",
      "'www.neuchatel.ch' not in the model\n",
      "'047' not in the model\n",
      "'francescos' not in the model\n",
      "'800' not in the model\n",
      "'19th' not in the model\n",
      "'82' not in the model\n",
      "'8.30' not in the model\n",
      "'1385' not in the model\n",
      "'1948' not in the model\n",
      "'rigi' not in the model\n",
      "'biasco' not in the model\n",
      "'pellanda' not in the model\n",
      "'meriot' not in the model\n",
      "'ticinese' not in the model\n",
      "'placad' not in the model\n",
      "'eye-max' not in the model\n",
      "'pamplat' not in the model\n",
      "'appliedapplied' not in the model\n",
      "'computurs' not in the model\n",
      "'1912' not in the model\n",
      "'1920' not in the model\n",
      "'percel' not in the model\n",
      "'1967' not in the model\n",
      "'1821' not in the model\n",
      "'brackenbury' not in the model\n",
      "'06' not in the model\n",
      "'8793888' not in the model\n",
      "'12:30' not in the model\n",
      "'robot-market' not in the model\n",
      "'elevenish' not in the model\n",
      "'pergam' not in the model\n",
      "'ornamentation' not in the model\n",
      "'picadelly' not in the model\n",
      "'4-star' not in the model\n",
      "'walton-on-thames' not in the model\n",
      "'glouster' not in the model\n",
      "'engoin' not in the model\n",
      "'ibendow' not in the model\n",
      "'whand' not in the model\n",
      "'1.00' not in the model\n",
      "'11.30' not in the model\n",
      "'38755980' not in the model\n",
      "'taxies' not in the model\n",
      "'101' not in the model\n",
      "'37' not in the model\n",
      "'reconvillier' not in the model\n",
      "'courtelary' not in the model\n",
      "'ogus' not in the model\n",
      "'micius' not in the model\n",
      "'circuited' not in the model\n",
      "'12.06.2001' not in the model\n",
      "'seventeenth-century' not in the model\n",
      "'semi-professors' not in the model\n",
      "'60th' not in the model\n",
      "'st-françois' not in the model\n",
      "'grand-chêne' not in the model\n",
      "'whitwell-on-the-hill' not in the model\n",
      "'843' not in the model\n",
      "'845' not in the model\n",
      "'driving-theory' not in the model\n",
      "'1953' not in the model\n",
      "'bloun' not in the model\n",
      "'five-week' not in the model\n",
      "'3-hour' not in the model\n",
      "'british-style' not in the model\n",
      "'1910' not in the model\n",
      "'a110' not in the model\n",
      "'a111' not in the model\n",
      "'a112' not in the model\n",
      "'dummont' not in the model\n",
      "'kei-shek' not in the model\n",
      "'populoniou' not in the model\n",
      "'centeral' not in the model\n",
      "'coppet' not in the model\n",
      "'divonne' not in the model\n",
      "'discoverers' not in the model\n",
      "'devinci' not in the model\n",
      "'gribaldi' not in the model\n",
      "'crariden' not in the model\n",
      "'barkeley' not in the model\n",
      "'12.06.01' not in the model\n",
      "'kongresshaus' not in the model\n",
      "'conference-organisation' not in the model\n",
      "'subway-station' not in the model\n",
      "'subway-train' not in the model\n",
      "'relationed' not in the model\n",
      "'41' not in the model\n",
      "'10.00-15.00' not in the model\n",
      "'city-walk' not in the model\n",
      "'denl-a-lion' not in the model\n",
      "'delamuraz' not in the model\n",
      "857 / 8759 tokens not in the model\n"
     ]
    }
   ],
   "source": [
    "# load the tokenized sentences from file, create the dictionary for embeddings look up\n",
    "token_to_embedding = {}\n",
    "not_pretrained = 0\n",
    "\n",
    "with open('tokenized.corrected.txt', 'r', encoding='utf-8') as textfile:\n",
    "    line = textfile.readline()\n",
    "    while line:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.split()\n",
    "        for token in line:\n",
    "            # for each unique token appeared in the training data, get a pretrained embedding \n",
    "            token = token.lower() # since Glove only encoded lower case embeddings\n",
    "            if token not in token_to_embedding.keys():\n",
    "                pretrained = glove_model.get_vector(token)\n",
    "                if pretrained == None:\n",
    "                    not_pretrained += 1\n",
    "                token_to_embedding[token.lower()] = pretrained\n",
    "        line = textfile.readline()\n",
    "        \n",
    "vocab_size = len(token_to_embedding)\n",
    "print('{} / {} tokens not in the model'.format(not_pretrained, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 / 130287 words having more than 15 dependencies not selected\n"
     ]
    }
   ],
   "source": [
    "# load all dependency pairs parsed using stanford core nlp 3.9.2 (2018-10-05 release), Enhanced++ dependencies were used\n",
    "\n",
    "# use governor word as center word\n",
    "# use dependent word as context word\n",
    "# skip-gram model input will be embeddings of governor, label will be embeddings of [dependent, governor]\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "dependency_listing = []\n",
    "max_length = 15\n",
    "not_selected = 0\n",
    "\n",
    "def TokensToEmbeddings(token_list, token_to_embedding):\n",
    "    output_list = []\n",
    "    for token in token_list:\n",
    "        try:\n",
    "            embedding = token_to_embedding[token]\n",
    "            if embedding == None:\n",
    "                embedding = [0 for i in range(EMBEDDING_DIM)]\n",
    "        except KeyError:\n",
    "            embedding = [0 for i in range(EMBEDDING_DIM)]\n",
    "        output_list.extend(embedding)\n",
    "    return output_list\n",
    "\n",
    "def TokenToEmbedding(token, token_to_embedding):\n",
    "    try:\n",
    "        embedding = token_to_embedding[token]\n",
    "        if embedding == None:\n",
    "            embedding = [0 for i in range(EMBEDDING_DIM)]\n",
    "    except KeyError:\n",
    "        embedding = [0 for i in range(EMBEDDING_DIM)]\n",
    "    return embedding\n",
    "\n",
    "with open('dependencies.corrected.txt', 'r', encoding='utf-8') as textfile:\n",
    "    metadata = json.load(textfile)\n",
    "    for sentence in metadata:\n",
    "        sentence_dependencies = {}\n",
    "        for dep_relation in sentence:\n",
    "            if dep_relation['dep'] == 'ROOT':\n",
    "                continue # skip the root token as it doesn't have any dependency\n",
    "            # word to predict is the governor, add all dependencies into the list of governor\n",
    "            governor = dep_relation['governorGloss'].lower()\n",
    "            dependent = dep_relation['dependentGloss'].lower()\n",
    "            if governor not in sentence_dependencies.keys():\n",
    "                sentence_dependencies[governor] = []\n",
    "            sentence_dependencies[governor].append(dependent)\n",
    "        for key, value in sentence_dependencies.items():\n",
    "            if len(value) > 15:\n",
    "                not_selected += 1\n",
    "                continue\n",
    "            dependency_listing.append([key, value])\n",
    "\n",
    "print('{} / {} words having more than 15 dependencies not selected'.format(not_selected, len(dependency_listing) + not_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = []\n",
    "batch_input = torch.empty((BATCH_SIZE, EMBEDDING_DIM * max_length))\n",
    "batch_label = torch.empty((BATCH_SIZE, EMBEDDING_DIM))\n",
    "counter = 0\n",
    "\n",
    "for (target, data) in dependency_listing:\n",
    "    counter += 1\n",
    "    label = TokenToEmbedding(target, token_to_embedding)\n",
    "    batch_label[counter - 1] = torch.FloatTensor(label)\n",
    "    \n",
    "    formatted_input = []\n",
    "    embeddings = TokensToEmbeddings(data, token_to_embedding)\n",
    "    formatted_input.extend(embeddings)\n",
    "    if len(data) < max_length:\n",
    "        # need padding if there are not enough dependencies\n",
    "        padding_length = (max_length - len(data)) * EMBEDDING_DIM\n",
    "        formatted_input.extend([0 for i in range(padding_length)])\n",
    "    batch_input[counter - 1] = torch.FloatTensor(formatted_input)\n",
    "    \n",
    "    if counter % BATCH_SIZE == 0:\n",
    "        training_input.append([batch_input, batch_label])\n",
    "        batch_input = torch.zeros((BATCH_SIZE, EMBEDDING_DIM * max_length))\n",
    "        batch_label = torch.empty((BATCH_SIZE, EMBEDDING_DIM))\n",
    "        counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508\n",
      "2\n",
      "torch.Size([256, 1500])\n"
     ]
    }
   ],
   "source": [
    "print(len(training_input)) # num_batches\n",
    "print(len(training_input[0])) # input, label\n",
    "print(training_input[0][0].shape) # batch shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0: 0 / 508 batches loss: 0.9563223719596863\n",
      "training epoch 0: 250 / 508 batches loss: 0.3582208454608917\n",
      "training epoch 0: 500 / 508 batches loss: 0.3946448862552643\n",
      "training epoch 0: avg epoch loss: 0.3823155462741852\n",
      "saving best model at epoch 0\n",
      "training epoch 1: 0 / 508 batches loss: 0.34397491812705994\n",
      "training epoch 1: 250 / 508 batches loss: 0.3492582440376282\n",
      "training epoch 1: 500 / 508 batches loss: 0.3903696835041046\n",
      "training epoch 1: avg epoch loss: 0.3640452027320862\n",
      "saving best model at epoch 1\n",
      "training epoch 2: 0 / 508 batches loss: 0.34080439805984497\n",
      "training epoch 2: 250 / 508 batches loss: 0.343002587556839\n",
      "training epoch 2: 500 / 508 batches loss: 0.38806384801864624\n",
      "training epoch 2: avg epoch loss: 0.35953980684280396\n",
      "saving best model at epoch 2\n",
      "training epoch 3: 0 / 508 batches loss: 0.33852696418762207\n",
      "training epoch 3: 250 / 508 batches loss: 0.33767566084861755\n",
      "training epoch 3: 500 / 508 batches loss: 0.38564181327819824\n",
      "training epoch 3: avg epoch loss: 0.35642772912979126\n",
      "saving best model at epoch 3\n",
      "training epoch 4: 0 / 508 batches loss: 0.3372371792793274\n",
      "training epoch 4: 250 / 508 batches loss: 0.3337031602859497\n",
      "training epoch 4: 500 / 508 batches loss: 0.38444212079048157\n",
      "training epoch 4: avg epoch loss: 0.35418421030044556\n",
      "saving best model at epoch 4\n",
      "training epoch 5: 0 / 508 batches loss: 0.3357955813407898\n",
      "training epoch 5: 250 / 508 batches loss: 0.3322046995162964\n",
      "training epoch 5: 500 / 508 batches loss: 0.38332870602607727\n",
      "training epoch 5: avg epoch loss: 0.3523796796798706\n",
      "saving best model at epoch 5\n",
      "training epoch 6: 0 / 508 batches loss: 0.33439332246780396\n",
      "training epoch 6: 250 / 508 batches loss: 0.3305388391017914\n",
      "training epoch 6: 500 / 508 batches loss: 0.3821084797382355\n",
      "training epoch 6: avg epoch loss: 0.35096442699432373\n",
      "saving best model at epoch 6\n",
      "training epoch 7: 0 / 508 batches loss: 0.333604633808136\n",
      "training epoch 7: 250 / 508 batches loss: 0.32812339067459106\n",
      "training epoch 7: 500 / 508 batches loss: 0.3807038366794586\n",
      "training epoch 7: avg epoch loss: 0.3496318757534027\n",
      "saving best model at epoch 7\n",
      "training epoch 8: 0 / 508 batches loss: 0.33288657665252686\n",
      "training epoch 8: 250 / 508 batches loss: 0.32716313004493713\n",
      "training epoch 8: 500 / 508 batches loss: 0.37945443391799927\n",
      "training epoch 8: avg epoch loss: 0.3484085202217102\n",
      "saving best model at epoch 8\n",
      "training epoch 9: 0 / 508 batches loss: 0.33282312750816345\n",
      "training epoch 9: 250 / 508 batches loss: 0.32637619972229004\n",
      "training epoch 9: 500 / 508 batches loss: 0.3787575364112854\n",
      "training epoch 9: avg epoch loss: 0.3473074734210968\n",
      "saving best model at epoch 9\n",
      "training epoch 10: 0 / 508 batches loss: 0.33194229006767273\n",
      "training epoch 10: 250 / 508 batches loss: 0.3251707851886749\n",
      "training epoch 10: 500 / 508 batches loss: 0.3775678873062134\n",
      "training epoch 10: avg epoch loss: 0.3462964594364166\n",
      "saving best model at epoch 10\n",
      "training epoch 11: 0 / 508 batches loss: 0.3312718868255615\n",
      "training epoch 11: 250 / 508 batches loss: 0.3234487771987915\n",
      "training epoch 11: 500 / 508 batches loss: 0.37685665488243103\n",
      "training epoch 11: avg epoch loss: 0.3453509211540222\n",
      "saving best model at epoch 11\n",
      "training epoch 12: 0 / 508 batches loss: 0.3302503824234009\n",
      "training epoch 12: 250 / 508 batches loss: 0.322334349155426\n",
      "training epoch 12: 500 / 508 batches loss: 0.37631934881210327\n",
      "training epoch 12: avg epoch loss: 0.34450653195381165\n",
      "saving best model at epoch 12\n",
      "training epoch 13: 0 / 508 batches loss: 0.3307327330112457\n",
      "training epoch 13: 250 / 508 batches loss: 0.32159748673439026\n",
      "training epoch 13: 500 / 508 batches loss: 0.37611261010169983\n",
      "training epoch 13: avg epoch loss: 0.3435467779636383\n",
      "saving best model at epoch 13\n",
      "training epoch 14: 0 / 508 batches loss: 0.32877442240715027\n",
      "training epoch 14: 250 / 508 batches loss: 0.32030999660491943\n",
      "training epoch 14: 500 / 508 batches loss: 0.37488242983818054\n",
      "training epoch 14: avg epoch loss: 0.34267038106918335\n",
      "saving best model at epoch 14\n",
      "training epoch 15: 0 / 508 batches loss: 0.32808244228363037\n",
      "training epoch 15: 250 / 508 batches loss: 0.3198063373565674\n",
      "training epoch 15: 500 / 508 batches loss: 0.3743133842945099\n",
      "training epoch 15: avg epoch loss: 0.3418833017349243\n",
      "saving best model at epoch 15\n",
      "training epoch 16: 0 / 508 batches loss: 0.32752323150634766\n",
      "training epoch 16: 250 / 508 batches loss: 0.3185329735279083\n",
      "training epoch 16: 500 / 508 batches loss: 0.3738862872123718\n",
      "training epoch 16: avg epoch loss: 0.34110715985298157\n",
      "saving best model at epoch 16\n",
      "training epoch 17: 0 / 508 batches loss: 0.32725000381469727\n",
      "training epoch 17: 250 / 508 batches loss: 0.3183126151561737\n",
      "training epoch 17: 500 / 508 batches loss: 0.3726309835910797\n",
      "training epoch 17: avg epoch loss: 0.34037038683891296\n",
      "saving best model at epoch 17\n",
      "training epoch 18: 0 / 508 batches loss: 0.32717013359069824\n",
      "training epoch 18: 250 / 508 batches loss: 0.3153764605522156\n",
      "training epoch 18: 500 / 508 batches loss: 0.3720703721046448\n",
      "training epoch 18: avg epoch loss: 0.3396424353122711\n",
      "saving best model at epoch 18\n",
      "training epoch 19: 0 / 508 batches loss: 0.32621362805366516\n",
      "training epoch 19: 250 / 508 batches loss: 0.3148021996021271\n",
      "training epoch 19: 500 / 508 batches loss: 0.3714178502559662\n",
      "training epoch 19: avg epoch loss: 0.3389497697353363\n",
      "saving best model at epoch 19\n",
      "training epoch 20: 0 / 508 batches loss: 0.32610294222831726\n",
      "training epoch 20: 250 / 508 batches loss: 0.31420016288757324\n",
      "training epoch 20: 500 / 508 batches loss: 0.37073254585266113\n",
      "training epoch 20: avg epoch loss: 0.338352233171463\n",
      "saving best model at epoch 20\n",
      "training epoch 21: 0 / 508 batches loss: 0.32505524158477783\n",
      "training epoch 21: 250 / 508 batches loss: 0.31372928619384766\n",
      "training epoch 21: 500 / 508 batches loss: 0.3701651990413666\n",
      "training epoch 21: avg epoch loss: 0.33771347999572754\n",
      "saving best model at epoch 21\n",
      "training epoch 22: 0 / 508 batches loss: 0.3243301808834076\n",
      "training epoch 22: 250 / 508 batches loss: 0.31329184770584106\n",
      "training epoch 22: 500 / 508 batches loss: 0.36898869276046753\n",
      "training epoch 22: avg epoch loss: 0.3371245265007019\n",
      "saving best model at epoch 22\n",
      "training epoch 23: 0 / 508 batches loss: 0.3236595094203949\n",
      "training epoch 23: 250 / 508 batches loss: 0.31270331144332886\n",
      "training epoch 23: 500 / 508 batches loss: 0.36890822649002075\n",
      "training epoch 23: avg epoch loss: 0.3365408778190613\n",
      "saving best model at epoch 23\n",
      "training epoch 24: 0 / 508 batches loss: 0.32330116629600525\n",
      "training epoch 24: 250 / 508 batches loss: 0.31233590841293335\n",
      "training epoch 24: 500 / 508 batches loss: 0.3678399324417114\n",
      "training epoch 24: avg epoch loss: 0.3359874486923218\n",
      "saving best model at epoch 24\n",
      "training epoch 25: 0 / 508 batches loss: 0.3229229748249054\n",
      "training epoch 25: 250 / 508 batches loss: 0.31178367137908936\n",
      "training epoch 25: 500 / 508 batches loss: 0.3676087260246277\n",
      "training epoch 25: avg epoch loss: 0.3354533314704895\n",
      "saving best model at epoch 25\n",
      "training epoch 26: 0 / 508 batches loss: 0.3227837085723877\n",
      "training epoch 26: 250 / 508 batches loss: 0.31152650713920593\n",
      "training epoch 26: 500 / 508 batches loss: 0.36681419610977173\n",
      "training epoch 26: avg epoch loss: 0.3350119888782501\n",
      "saving best model at epoch 26\n",
      "training epoch 27: 0 / 508 batches loss: 0.32229533791542053\n",
      "training epoch 27: 250 / 508 batches loss: 0.3115340769290924\n",
      "training epoch 27: 500 / 508 batches loss: 0.3667198419570923\n",
      "training epoch 27: avg epoch loss: 0.3346084952354431\n",
      "saving best model at epoch 27\n",
      "training epoch 28: 0 / 508 batches loss: 0.32191595435142517\n",
      "training epoch 28: 250 / 508 batches loss: 0.3111901879310608\n",
      "training epoch 28: 500 / 508 batches loss: 0.3664166331291199\n",
      "training epoch 28: avg epoch loss: 0.3341894745826721\n",
      "saving best model at epoch 28\n",
      "training epoch 29: 0 / 508 batches loss: 0.32117369771003723\n",
      "training epoch 29: 250 / 508 batches loss: 0.310834676027298\n",
      "training epoch 29: 500 / 508 batches loss: 0.36611878871917725\n",
      "training epoch 29: avg epoch loss: 0.33377423882484436\n",
      "saving best model at epoch 29\n",
      "training epoch 30: 0 / 508 batches loss: 0.320421040058136\n",
      "training epoch 30: 250 / 508 batches loss: 0.3102107644081116\n",
      "training epoch 30: 500 / 508 batches loss: 0.3651481866836548\n",
      "training epoch 30: avg epoch loss: 0.3333321213722229\n",
      "saving best model at epoch 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 31: 0 / 508 batches loss: 0.3201550543308258\n",
      "training epoch 31: 250 / 508 batches loss: 0.3100365400314331\n",
      "training epoch 31: 500 / 508 batches loss: 0.3649429976940155\n",
      "training epoch 31: avg epoch loss: 0.33290788531303406\n",
      "saving best model at epoch 31\n",
      "training epoch 32: 0 / 508 batches loss: 0.3199741840362549\n",
      "training epoch 32: 250 / 508 batches loss: 0.30972909927368164\n",
      "training epoch 32: 500 / 508 batches loss: 0.3651171624660492\n",
      "training epoch 32: avg epoch loss: 0.332588255405426\n",
      "saving best model at epoch 32\n",
      "training epoch 33: 0 / 508 batches loss: 0.31970301270484924\n",
      "training epoch 33: 250 / 508 batches loss: 0.30954957008361816\n",
      "training epoch 33: 500 / 508 batches loss: 0.36422476172447205\n",
      "training epoch 33: avg epoch loss: 0.3322709798812866\n",
      "saving best model at epoch 33\n",
      "training epoch 34: 0 / 508 batches loss: 0.3192899227142334\n",
      "training epoch 34: 250 / 508 batches loss: 0.3094882369041443\n",
      "training epoch 34: 500 / 508 batches loss: 0.36363688111305237\n",
      "training epoch 34: avg epoch loss: 0.3319583833217621\n",
      "saving best model at epoch 34\n",
      "training epoch 35: 0 / 508 batches loss: 0.31890949606895447\n",
      "training epoch 35: 250 / 508 batches loss: 0.3093648850917816\n",
      "training epoch 35: 500 / 508 batches loss: 0.363391637802124\n",
      "training epoch 35: avg epoch loss: 0.3315790891647339\n",
      "saving best model at epoch 35\n",
      "training epoch 36: 0 / 508 batches loss: 0.31885799765586853\n",
      "training epoch 36: 250 / 508 batches loss: 0.3089304566383362\n",
      "training epoch 36: 500 / 508 batches loss: 0.3630671203136444\n",
      "training epoch 36: avg epoch loss: 0.3313296139240265\n",
      "saving best model at epoch 36\n",
      "training epoch 37: 0 / 508 batches loss: 0.3186453878879547\n",
      "training epoch 37: 250 / 508 batches loss: 0.30883491039276123\n",
      "training epoch 37: 500 / 508 batches loss: 0.3624345362186432\n",
      "training epoch 37: avg epoch loss: 0.33110564947128296\n",
      "saving best model at epoch 37\n",
      "training epoch 38: 0 / 508 batches loss: 0.3179560601711273\n",
      "training epoch 38: 250 / 508 batches loss: 0.30885547399520874\n",
      "training epoch 38: 500 / 508 batches loss: 0.36244073510169983\n",
      "training epoch 38: avg epoch loss: 0.3307761549949646\n",
      "saving best model at epoch 38\n",
      "training epoch 39: 0 / 508 batches loss: 0.3177756369113922\n",
      "training epoch 39: 250 / 508 batches loss: 0.30852532386779785\n",
      "training epoch 39: 500 / 508 batches loss: 0.3619861602783203\n",
      "training epoch 39: avg epoch loss: 0.3304578363895416\n",
      "saving best model at epoch 39\n",
      "training epoch 40: 0 / 508 batches loss: 0.31775838136672974\n",
      "training epoch 40: 250 / 508 batches loss: 0.3080209493637085\n",
      "training epoch 40: 500 / 508 batches loss: 0.3620336055755615\n",
      "training epoch 40: avg epoch loss: 0.3301902115345001\n",
      "saving best model at epoch 40\n",
      "training epoch 41: 0 / 508 batches loss: 0.317118763923645\n",
      "training epoch 41: 250 / 508 batches loss: 0.3077465891838074\n",
      "training epoch 41: 500 / 508 batches loss: 0.3618950843811035\n",
      "training epoch 41: avg epoch loss: 0.3299403190612793\n",
      "saving best model at epoch 41\n",
      "training epoch 42: 0 / 508 batches loss: 0.31720179319381714\n",
      "training epoch 42: 250 / 508 batches loss: 0.3078821301460266\n",
      "training epoch 42: 500 / 508 batches loss: 0.36107152700424194\n",
      "training epoch 42: avg epoch loss: 0.32972297072410583\n",
      "saving best model at epoch 42\n",
      "training epoch 43: 0 / 508 batches loss: 0.316854327917099\n",
      "training epoch 43: 250 / 508 batches loss: 0.3076661229133606\n",
      "training epoch 43: 500 / 508 batches loss: 0.3610296845436096\n",
      "training epoch 43: avg epoch loss: 0.32949143648147583\n",
      "saving best model at epoch 43\n",
      "training epoch 44: 0 / 508 batches loss: 0.31636884808540344\n",
      "training epoch 44: 250 / 508 batches loss: 0.30687978863716125\n",
      "training epoch 44: 500 / 508 batches loss: 0.36063113808631897\n",
      "training epoch 44: avg epoch loss: 0.32922419905662537\n",
      "saving best model at epoch 44\n",
      "training epoch 45: 0 / 508 batches loss: 0.3166978061199188\n",
      "training epoch 45: 250 / 508 batches loss: 0.3071771562099457\n",
      "training epoch 45: 500 / 508 batches loss: 0.3599885106086731\n",
      "training epoch 45: avg epoch loss: 0.32901903986930847\n",
      "saving best model at epoch 45\n",
      "training epoch 46: 0 / 508 batches loss: 0.31623607873916626\n",
      "training epoch 46: 250 / 508 batches loss: 0.3068076968193054\n",
      "training epoch 46: 500 / 508 batches loss: 0.36040687561035156\n",
      "training epoch 46: avg epoch loss: 0.3287695348262787\n",
      "saving best model at epoch 46\n",
      "training epoch 47: 0 / 508 batches loss: 0.31592753529548645\n",
      "training epoch 47: 250 / 508 batches loss: 0.30676019191741943\n",
      "training epoch 47: 500 / 508 batches loss: 0.35994625091552734\n",
      "training epoch 47: avg epoch loss: 0.3285861611366272\n",
      "saving best model at epoch 47\n",
      "training epoch 48: 0 / 508 batches loss: 0.3156454265117645\n",
      "training epoch 48: 250 / 508 batches loss: 0.3064805567264557\n",
      "training epoch 48: 500 / 508 batches loss: 0.36022984981536865\n",
      "training epoch 48: avg epoch loss: 0.32839179039001465\n",
      "saving best model at epoch 48\n",
      "training epoch 49: 0 / 508 batches loss: 0.3155929148197174\n",
      "training epoch 49: 250 / 508 batches loss: 0.3069404363632202\n",
      "training epoch 49: 500 / 508 batches loss: 0.36014729738235474\n",
      "training epoch 49: avg epoch loss: 0.32822805643081665\n",
      "saving best model at epoch 49\n"
     ]
    }
   ],
   "source": [
    "# running with relu activation with softmax (modifications made in models.py)\n",
    "num_epochs = 50\n",
    "torch.manual_seed(32)\n",
    "epoch_loss = torch.zeros(num_epochs)\n",
    "num_batches = len(training_input)\n",
    "\n",
    "model_nn = NN(EMBEDDING_DIM * max_length, EMBEDDING_DIM)\n",
    "learning_rate = 0.01    \n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=learning_rate)\n",
    "best_model = 999\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    model_nn.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(training_input):\n",
    "        current_batch_size = target.size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model_nn(data)\n",
    "        mask = Variable(torch.ones(current_batch_size), requires_grad=False)\n",
    "        loss = F.cosine_embedding_loss(output, target, mask) # the model uses cosine embedding loss\n",
    "        total_epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 250 == 0:\n",
    "            print('training epoch {}: {} / {} batches loss: {}'.format(epoch, batch_idx, num_batches, loss))\n",
    "        \n",
    "    epoch_loss[epoch] = total_epoch_loss / num_batches\n",
    "    print('training epoch {}: avg epoch loss: {}'.format(epoch, epoch_loss[epoch]))\n",
    "    if epoch_loss[epoch] < best_model:\n",
    "        best_model = epoch_loss[epoch]\n",
    "        print('saving best model at epoch {}'.format(epoch))\n",
    "        torch.save(model_nn.state_dict, 'nn.softmax.best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0: 0 / 508 batches loss: 0.9563081860542297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0: 250 / 508 batches loss: 0.36887335777282715\n",
      "training epoch 0: 500 / 508 batches loss: 0.40389418601989746\n",
      "training epoch 0: avg epoch loss: 0.39936333894729614\n",
      "saving best model at epoch 0\n",
      "training epoch 1: 0 / 508 batches loss: 0.35683301091194153\n",
      "training epoch 1: 250 / 508 batches loss: 0.35543352365493774\n",
      "training epoch 1: 500 / 508 batches loss: 0.3989146947860718\n",
      "training epoch 1: avg epoch loss: 0.3762851655483246\n",
      "saving best model at epoch 1\n",
      "training epoch 2: 0 / 508 batches loss: 0.3504186272621155\n",
      "training epoch 2: 250 / 508 batches loss: 0.3507583439350128\n",
      "training epoch 2: 500 / 508 batches loss: 0.39630061388015747\n",
      "training epoch 2: avg epoch loss: 0.37159040570259094\n",
      "saving best model at epoch 2\n",
      "training epoch 3: 0 / 508 batches loss: 0.3473049998283386\n",
      "training epoch 3: 250 / 508 batches loss: 0.3481609523296356\n",
      "training epoch 3: 500 / 508 batches loss: 0.39462393522262573\n",
      "training epoch 3: avg epoch loss: 0.36883634328842163\n",
      "saving best model at epoch 3\n",
      "training epoch 4: 0 / 508 batches loss: 0.34547722339630127\n",
      "training epoch 4: 250 / 508 batches loss: 0.3460685610771179\n",
      "training epoch 4: 500 / 508 batches loss: 0.393133282661438\n",
      "training epoch 4: avg epoch loss: 0.36687010526657104\n",
      "saving best model at epoch 4\n",
      "training epoch 5: 0 / 508 batches loss: 0.3445627689361572\n",
      "training epoch 5: 250 / 508 batches loss: 0.3446357846260071\n",
      "training epoch 5: 500 / 508 batches loss: 0.3921733796596527\n",
      "training epoch 5: avg epoch loss: 0.36533069610595703\n",
      "saving best model at epoch 5\n",
      "training epoch 6: 0 / 508 batches loss: 0.3431357145309448\n",
      "training epoch 6: 250 / 508 batches loss: 0.34361574053764343\n",
      "training epoch 6: 500 / 508 batches loss: 0.3915984332561493\n",
      "training epoch 6: avg epoch loss: 0.3642856478691101\n",
      "saving best model at epoch 6\n",
      "training epoch 7: 0 / 508 batches loss: 0.3425167202949524\n",
      "training epoch 7: 250 / 508 batches loss: 0.3419703245162964\n",
      "training epoch 7: 500 / 508 batches loss: 0.3915826976299286\n",
      "training epoch 7: avg epoch loss: 0.3630935549736023\n",
      "saving best model at epoch 7\n",
      "training epoch 8: 0 / 508 batches loss: 0.3413870930671692\n",
      "training epoch 8: 250 / 508 batches loss: 0.34096992015838623\n",
      "training epoch 8: 500 / 508 batches loss: 0.39036667346954346\n",
      "training epoch 8: avg epoch loss: 0.3620952367782593\n",
      "saving best model at epoch 8\n",
      "training epoch 9: 0 / 508 batches loss: 0.3409595191478729\n",
      "training epoch 9: 250 / 508 batches loss: 0.34014832973480225\n",
      "training epoch 9: 500 / 508 batches loss: 0.3896718919277191\n",
      "training epoch 9: avg epoch loss: 0.36128413677215576\n",
      "saving best model at epoch 9\n",
      "training epoch 10: 0 / 508 batches loss: 0.3407047390937805\n",
      "training epoch 10: 250 / 508 batches loss: 0.3390756845474243\n",
      "training epoch 10: 500 / 508 batches loss: 0.38884347677230835\n",
      "training epoch 10: avg epoch loss: 0.3605675995349884\n",
      "saving best model at epoch 10\n",
      "training epoch 11: 0 / 508 batches loss: 0.3400750160217285\n",
      "training epoch 11: 250 / 508 batches loss: 0.3383345603942871\n",
      "training epoch 11: 500 / 508 batches loss: 0.3886300027370453\n",
      "training epoch 11: avg epoch loss: 0.35992223024368286\n",
      "saving best model at epoch 11\n",
      "training epoch 12: 0 / 508 batches loss: 0.3396885395050049\n",
      "training epoch 12: 250 / 508 batches loss: 0.33753839135169983\n",
      "training epoch 12: 500 / 508 batches loss: 0.38778048753738403\n",
      "training epoch 12: avg epoch loss: 0.35930123925209045\n",
      "saving best model at epoch 12\n",
      "training epoch 13: 0 / 508 batches loss: 0.33943498134613037\n",
      "training epoch 13: 250 / 508 batches loss: 0.337028443813324\n",
      "training epoch 13: 500 / 508 batches loss: 0.3876565992832184\n",
      "training epoch 13: avg epoch loss: 0.35876932740211487\n",
      "saving best model at epoch 13\n",
      "training epoch 14: 0 / 508 batches loss: 0.3391852080821991\n",
      "training epoch 14: 250 / 508 batches loss: 0.3366709351539612\n",
      "training epoch 14: 500 / 508 batches loss: 0.38747596740722656\n",
      "training epoch 14: avg epoch loss: 0.35833314061164856\n",
      "saving best model at epoch 14\n",
      "training epoch 15: 0 / 508 batches loss: 0.33904412388801575\n",
      "training epoch 15: 250 / 508 batches loss: 0.3358968198299408\n",
      "training epoch 15: 500 / 508 batches loss: 0.3869025409221649\n",
      "training epoch 15: avg epoch loss: 0.3577849566936493\n",
      "saving best model at epoch 15\n",
      "training epoch 16: 0 / 508 batches loss: 0.3387545347213745\n",
      "training epoch 16: 250 / 508 batches loss: 0.33572062849998474\n",
      "training epoch 16: 500 / 508 batches loss: 0.38693177700042725\n",
      "training epoch 16: avg epoch loss: 0.35730111598968506\n",
      "saving best model at epoch 16\n",
      "training epoch 17: 0 / 508 batches loss: 0.3385683000087738\n",
      "training epoch 17: 250 / 508 batches loss: 0.33483949303627014\n",
      "training epoch 17: 500 / 508 batches loss: 0.38669222593307495\n",
      "training epoch 17: avg epoch loss: 0.35695841908454895\n",
      "saving best model at epoch 17\n",
      "training epoch 18: 0 / 508 batches loss: 0.3376341462135315\n",
      "training epoch 18: 250 / 508 batches loss: 0.33439040184020996\n",
      "training epoch 18: 500 / 508 batches loss: 0.38636937737464905\n",
      "training epoch 18: avg epoch loss: 0.3565252423286438\n",
      "saving best model at epoch 18\n",
      "training epoch 19: 0 / 508 batches loss: 0.3387313187122345\n",
      "training epoch 19: 250 / 508 batches loss: 0.33445003628730774\n",
      "training epoch 19: 500 / 508 batches loss: 0.3859933912754059\n",
      "training epoch 19: avg epoch loss: 0.3561367392539978\n",
      "saving best model at epoch 19\n",
      "training epoch 20: 0 / 508 batches loss: 0.337759405374527\n",
      "training epoch 20: 250 / 508 batches loss: 0.33376622200012207\n",
      "training epoch 20: 500 / 508 batches loss: 0.38581162691116333\n",
      "training epoch 20: avg epoch loss: 0.3557829260826111\n",
      "saving best model at epoch 20\n",
      "training epoch 21: 0 / 508 batches loss: 0.3379354774951935\n",
      "training epoch 21: 250 / 508 batches loss: 0.3333323299884796\n",
      "training epoch 21: 500 / 508 batches loss: 0.3857404887676239\n",
      "training epoch 21: avg epoch loss: 0.3555012047290802\n",
      "saving best model at epoch 21\n",
      "training epoch 22: 0 / 508 batches loss: 0.33781999349594116\n",
      "training epoch 22: 250 / 508 batches loss: 0.33333319425582886\n",
      "training epoch 22: 500 / 508 batches loss: 0.3860435485839844\n",
      "training epoch 22: avg epoch loss: 0.35521215200424194\n",
      "saving best model at epoch 22\n",
      "training epoch 23: 0 / 508 batches loss: 0.3375547528266907\n",
      "training epoch 23: 250 / 508 batches loss: 0.3329038619995117\n",
      "training epoch 23: 500 / 508 batches loss: 0.3852148652076721\n",
      "training epoch 23: avg epoch loss: 0.35494938492774963\n",
      "saving best model at epoch 23\n",
      "training epoch 24: 0 / 508 batches loss: 0.33697569370269775\n",
      "training epoch 24: 250 / 508 batches loss: 0.3324662446975708\n",
      "training epoch 24: 500 / 508 batches loss: 0.3850020468235016\n",
      "training epoch 24: avg epoch loss: 0.35462111234664917\n",
      "saving best model at epoch 24\n",
      "training epoch 25: 0 / 508 batches loss: 0.3372134268283844\n",
      "training epoch 25: 250 / 508 batches loss: 0.3323337435722351\n",
      "training epoch 25: 500 / 508 batches loss: 0.38503891229629517\n",
      "training epoch 25: avg epoch loss: 0.3544331192970276\n",
      "saving best model at epoch 25\n",
      "training epoch 26: 0 / 508 batches loss: 0.3372265100479126\n",
      "training epoch 26: 250 / 508 batches loss: 0.33179977536201477\n",
      "training epoch 26: 500 / 508 batches loss: 0.38557395339012146\n",
      "training epoch 26: avg epoch loss: 0.3541814088821411\n",
      "saving best model at epoch 26\n",
      "training epoch 27: 0 / 508 batches loss: 0.3368922770023346\n",
      "training epoch 27: 250 / 508 batches loss: 0.3317447602748871\n",
      "training epoch 27: 500 / 508 batches loss: 0.38474780321121216\n",
      "training epoch 27: avg epoch loss: 0.3539448082447052\n",
      "saving best model at epoch 27\n",
      "training epoch 28: 0 / 508 batches loss: 0.3363235294818878\n",
      "training epoch 28: 250 / 508 batches loss: 0.3315375745296478\n",
      "training epoch 28: 500 / 508 batches loss: 0.3843632936477661\n",
      "training epoch 28: avg epoch loss: 0.35371026396751404\n",
      "saving best model at epoch 28\n",
      "training epoch 29: 0 / 508 batches loss: 0.3363645672798157\n",
      "training epoch 29: 250 / 508 batches loss: 0.33143654465675354\n",
      "training epoch 29: 500 / 508 batches loss: 0.38412395119667053\n",
      "training epoch 29: avg epoch loss: 0.3534887731075287\n",
      "saving best model at epoch 29\n",
      "training epoch 30: 0 / 508 batches loss: 0.3375242352485657\n",
      "training epoch 30: 250 / 508 batches loss: 0.3310414254665375\n",
      "training epoch 30: 500 / 508 batches loss: 0.3844652771949768\n",
      "training epoch 30: avg epoch loss: 0.35330522060394287\n",
      "saving best model at epoch 30\n",
      "training epoch 31: 0 / 508 batches loss: 0.33658814430236816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 31: 250 / 508 batches loss: 0.3309951424598694\n",
      "training epoch 31: 500 / 508 batches loss: 0.3842032253742218\n",
      "training epoch 31: avg epoch loss: 0.3531019687652588\n",
      "saving best model at epoch 31\n",
      "training epoch 32: 0 / 508 batches loss: 0.3363994359970093\n",
      "training epoch 32: 250 / 508 batches loss: 0.33080706000328064\n",
      "training epoch 32: 500 / 508 batches loss: 0.38410618901252747\n",
      "training epoch 32: avg epoch loss: 0.3529800772666931\n",
      "saving best model at epoch 32\n",
      "training epoch 33: 0 / 508 batches loss: 0.3358014225959778\n",
      "training epoch 33: 250 / 508 batches loss: 0.3302411139011383\n",
      "training epoch 33: 500 / 508 batches loss: 0.3836556077003479\n",
      "training epoch 33: avg epoch loss: 0.3527534604072571\n",
      "saving best model at epoch 33\n",
      "training epoch 34: 0 / 508 batches loss: 0.33552777767181396\n",
      "training epoch 34: 250 / 508 batches loss: 0.330337792634964\n",
      "training epoch 34: 500 / 508 batches loss: 0.3835183084011078\n",
      "training epoch 34: avg epoch loss: 0.3526175916194916\n",
      "saving best model at epoch 34\n",
      "training epoch 35: 0 / 508 batches loss: 0.3363841772079468\n",
      "training epoch 35: 250 / 508 batches loss: 0.32993969321250916\n",
      "training epoch 35: 500 / 508 batches loss: 0.3836231529712677\n",
      "training epoch 35: avg epoch loss: 0.3524416983127594\n",
      "saving best model at epoch 35\n",
      "training epoch 36: 0 / 508 batches loss: 0.33607834577560425\n",
      "training epoch 36: 250 / 508 batches loss: 0.33009180426597595\n",
      "training epoch 36: 500 / 508 batches loss: 0.38338226079940796\n",
      "training epoch 36: avg epoch loss: 0.35241255164146423\n",
      "saving best model at epoch 36\n",
      "training epoch 37: 0 / 508 batches loss: 0.33586204051971436\n",
      "training epoch 37: 250 / 508 batches loss: 0.329911470413208\n",
      "training epoch 37: 500 / 508 batches loss: 0.3828362226486206\n",
      "training epoch 37: avg epoch loss: 0.35213392972946167\n",
      "saving best model at epoch 37\n",
      "training epoch 38: 0 / 508 batches loss: 0.33539846539497375\n",
      "training epoch 38: 250 / 508 batches loss: 0.3295910060405731\n",
      "training epoch 38: 500 / 508 batches loss: 0.38286328315734863\n",
      "training epoch 38: avg epoch loss: 0.3520091474056244\n",
      "saving best model at epoch 38\n",
      "training epoch 39: 0 / 508 batches loss: 0.3355189561843872\n",
      "training epoch 39: 250 / 508 batches loss: 0.329306036233902\n",
      "training epoch 39: 500 / 508 batches loss: 0.3827590048313141\n",
      "training epoch 39: avg epoch loss: 0.35184335708618164\n",
      "saving best model at epoch 39\n",
      "training epoch 40: 0 / 508 batches loss: 0.3349853456020355\n",
      "training epoch 40: 250 / 508 batches loss: 0.32929298281669617\n",
      "training epoch 40: 500 / 508 batches loss: 0.3824923634529114\n",
      "training epoch 40: avg epoch loss: 0.35168758034706116\n",
      "saving best model at epoch 40\n",
      "training epoch 41: 0 / 508 batches loss: 0.33477312326431274\n",
      "training epoch 41: 250 / 508 batches loss: 0.3291573226451874\n",
      "training epoch 41: 500 / 508 batches loss: 0.38319364190101624\n",
      "training epoch 41: avg epoch loss: 0.3515327274799347\n",
      "saving best model at epoch 41\n",
      "training epoch 42: 0 / 508 batches loss: 0.3348991870880127\n",
      "training epoch 42: 250 / 508 batches loss: 0.3288915455341339\n",
      "training epoch 42: 500 / 508 batches loss: 0.3826160132884979\n",
      "training epoch 42: avg epoch loss: 0.35137781500816345\n",
      "saving best model at epoch 42\n",
      "training epoch 43: 0 / 508 batches loss: 0.3350176513195038\n",
      "training epoch 43: 250 / 508 batches loss: 0.32901519536972046\n",
      "training epoch 43: 500 / 508 batches loss: 0.3825078308582306\n",
      "training epoch 43: avg epoch loss: 0.3512566089630127\n",
      "saving best model at epoch 43\n",
      "training epoch 44: 0 / 508 batches loss: 0.3345019221305847\n",
      "training epoch 44: 250 / 508 batches loss: 0.32909733057022095\n",
      "training epoch 44: 500 / 508 batches loss: 0.38226789236068726\n",
      "training epoch 44: avg epoch loss: 0.35114607214927673\n",
      "saving best model at epoch 44\n",
      "training epoch 45: 0 / 508 batches loss: 0.33475083112716675\n",
      "training epoch 45: 250 / 508 batches loss: 0.3284524977207184\n",
      "training epoch 45: 500 / 508 batches loss: 0.3822859823703766\n",
      "training epoch 45: avg epoch loss: 0.3510638177394867\n",
      "saving best model at epoch 45\n",
      "training epoch 46: 0 / 508 batches loss: 0.334432989358902\n",
      "training epoch 46: 250 / 508 batches loss: 0.3281939923763275\n",
      "training epoch 46: 500 / 508 batches loss: 0.38141173124313354\n",
      "training epoch 46: avg epoch loss: 0.35092172026634216\n",
      "saving best model at epoch 46\n",
      "training epoch 47: 0 / 508 batches loss: 0.3335181176662445\n",
      "training epoch 47: 250 / 508 batches loss: 0.32800063490867615\n",
      "training epoch 47: 500 / 508 batches loss: 0.38189366459846497\n",
      "training epoch 47: avg epoch loss: 0.3508440852165222\n",
      "saving best model at epoch 47\n",
      "training epoch 48: 0 / 508 batches loss: 0.3342842161655426\n",
      "training epoch 48: 250 / 508 batches loss: 0.3279123306274414\n",
      "training epoch 48: 500 / 508 batches loss: 0.3816516101360321\n",
      "training epoch 48: avg epoch loss: 0.35070714354515076\n",
      "saving best model at epoch 48\n",
      "training epoch 49: 0 / 508 batches loss: 0.33373746275901794\n",
      "training epoch 49: 250 / 508 batches loss: 0.3275958001613617\n",
      "training epoch 49: 500 / 508 batches loss: 0.38199013471603394\n",
      "training epoch 49: avg epoch loss: 0.35057276487350464\n",
      "saving best model at epoch 49\n"
     ]
    }
   ],
   "source": [
    "# running with tanh activation with softmax (modifications made in models.py)\n",
    "num_epochs = 50\n",
    "torch.manual_seed(32)\n",
    "epoch_loss = torch.zeros(num_epochs)\n",
    "num_batches = len(training_input)\n",
    "\n",
    "model_nn = NN(EMBEDDING_DIM * max_length, EMBEDDING_DIM)\n",
    "learning_rate = 0.01    \n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=learning_rate)\n",
    "best_model = 999\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    model_nn.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(training_input):\n",
    "        current_batch_size = target.size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model_nn(data)\n",
    "        mask = Variable(torch.ones(current_batch_size), requires_grad=False)\n",
    "        loss = F.cosine_embedding_loss(output, target, mask) # the model uses cosine embedding loss\n",
    "        total_epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 250 == 0:\n",
    "            print('training epoch {}: {} / {} batches loss: {}'.format(epoch, batch_idx, num_batches, loss))\n",
    "\n",
    "    epoch_loss[epoch] = total_epoch_loss / num_batches\n",
    "    print('training epoch {}: avg epoch loss: {}'.format(epoch, epoch_loss[epoch]))\n",
    "    if epoch_loss[epoch] < best_model:\n",
    "        best_model = epoch_loss[epoch]\n",
    "        print('saving best model at epoch {}'.format(epoch))\n",
    "        torch.save(model_nn.state_dict, 'nn.tanh.best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0: 0 / 508 batches loss: 1.0272125005722046\n",
      "training epoch 0: 250 / 508 batches loss: 0.2708292305469513\n",
      "training epoch 0: 500 / 508 batches loss: 0.30863702297210693\n",
      "training epoch 0: avg epoch loss: 0.29028478264808655\n",
      "saving best model at epoch 0\n",
      "training epoch 1: 0 / 508 batches loss: 0.2718968987464905\n",
      "training epoch 1: 250 / 508 batches loss: 0.24587306380271912\n",
      "training epoch 1: 500 / 508 batches loss: 0.2986571192741394\n",
      "training epoch 1: avg epoch loss: 0.26780298352241516\n",
      "saving best model at epoch 1\n",
      "training epoch 2: 0 / 508 batches loss: 0.26715633273124695\n",
      "training epoch 2: 250 / 508 batches loss: 0.23650933802127838\n",
      "training epoch 2: 500 / 508 batches loss: 0.29206669330596924\n",
      "training epoch 2: avg epoch loss: 0.2598472833633423\n",
      "saving best model at epoch 2\n",
      "training epoch 3: 0 / 508 batches loss: 0.2639191746711731\n",
      "training epoch 3: 250 / 508 batches loss: 0.2306453287601471\n",
      "training epoch 3: 500 / 508 batches loss: 0.28679659962654114\n",
      "training epoch 3: avg epoch loss: 0.25444501638412476\n",
      "saving best model at epoch 3\n",
      "training epoch 4: 0 / 508 batches loss: 0.26012179255485535\n",
      "training epoch 4: 250 / 508 batches loss: 0.22654278576374054\n",
      "training epoch 4: 500 / 508 batches loss: 0.284352570772171\n",
      "training epoch 4: avg epoch loss: 0.25039011240005493\n",
      "saving best model at epoch 4\n",
      "training epoch 5: 0 / 508 batches loss: 0.25732144713401794\n",
      "training epoch 5: 250 / 508 batches loss: 0.2238369882106781\n",
      "training epoch 5: 500 / 508 batches loss: 0.28147223591804504\n",
      "training epoch 5: avg epoch loss: 0.2471449375152588\n",
      "saving best model at epoch 5\n",
      "training epoch 6: 0 / 508 batches loss: 0.25388026237487793\n",
      "training epoch 6: 250 / 508 batches loss: 0.22159011662006378\n",
      "training epoch 6: 500 / 508 batches loss: 0.2786879539489746\n",
      "training epoch 6: avg epoch loss: 0.24439656734466553\n",
      "saving best model at epoch 6\n",
      "training epoch 7: 0 / 508 batches loss: 0.2505492568016052\n",
      "training epoch 7: 250 / 508 batches loss: 0.22010481357574463\n",
      "training epoch 7: 500 / 508 batches loss: 0.27598702907562256\n",
      "training epoch 7: avg epoch loss: 0.2420937418937683\n",
      "saving best model at epoch 7\n",
      "training epoch 8: 0 / 508 batches loss: 0.2486695498228073\n",
      "training epoch 8: 250 / 508 batches loss: 0.21791377663612366\n",
      "training epoch 8: 500 / 508 batches loss: 0.2745313048362732\n",
      "training epoch 8: avg epoch loss: 0.24003151059150696\n",
      "saving best model at epoch 8\n",
      "training epoch 9: 0 / 508 batches loss: 0.24594156444072723\n",
      "training epoch 9: 250 / 508 batches loss: 0.2163463830947876\n",
      "training epoch 9: 500 / 508 batches loss: 0.27267029881477356\n",
      "training epoch 9: avg epoch loss: 0.23818518221378326\n",
      "saving best model at epoch 9\n",
      "training epoch 10: 0 / 508 batches loss: 0.24283908307552338\n",
      "training epoch 10: 250 / 508 batches loss: 0.21528078615665436\n",
      "training epoch 10: 500 / 508 batches loss: 0.2711431682109833\n",
      "training epoch 10: avg epoch loss: 0.23643140494823456\n",
      "saving best model at epoch 10\n",
      "training epoch 11: 0 / 508 batches loss: 0.2407824993133545\n",
      "training epoch 11: 250 / 508 batches loss: 0.21362754702568054\n",
      "training epoch 11: 500 / 508 batches loss: 0.26978379487991333\n",
      "training epoch 11: avg epoch loss: 0.23489953577518463\n",
      "saving best model at epoch 11\n",
      "training epoch 12: 0 / 508 batches loss: 0.2390436977148056\n",
      "training epoch 12: 250 / 508 batches loss: 0.2117568701505661\n",
      "training epoch 12: 500 / 508 batches loss: 0.26894161105155945\n",
      "training epoch 12: avg epoch loss: 0.23343463242053986\n",
      "saving best model at epoch 12\n",
      "training epoch 13: 0 / 508 batches loss: 0.23633979260921478\n",
      "training epoch 13: 250 / 508 batches loss: 0.21090152859687805\n",
      "training epoch 13: 500 / 508 batches loss: 0.26848506927490234\n",
      "training epoch 13: avg epoch loss: 0.23214223980903625\n",
      "saving best model at epoch 13\n",
      "training epoch 14: 0 / 508 batches loss: 0.23414184153079987\n",
      "training epoch 14: 250 / 508 batches loss: 0.20942655205726624\n",
      "training epoch 14: 500 / 508 batches loss: 0.26662951707839966\n",
      "training epoch 14: avg epoch loss: 0.23090830445289612\n",
      "saving best model at epoch 14\n",
      "training epoch 15: 0 / 508 batches loss: 0.23176632821559906\n",
      "training epoch 15: 250 / 508 batches loss: 0.20904424786567688\n",
      "training epoch 15: 500 / 508 batches loss: 0.26562726497650146\n",
      "training epoch 15: avg epoch loss: 0.2297956645488739\n",
      "saving best model at epoch 15\n",
      "training epoch 16: 0 / 508 batches loss: 0.22994257509708405\n",
      "training epoch 16: 250 / 508 batches loss: 0.20875944197177887\n",
      "training epoch 16: 500 / 508 batches loss: 0.2655276954174042\n",
      "training epoch 16: avg epoch loss: 0.22876879572868347\n",
      "saving best model at epoch 16\n",
      "training epoch 17: 0 / 508 batches loss: 0.22938694059848785\n",
      "training epoch 17: 250 / 508 batches loss: 0.2078453153371811\n",
      "training epoch 17: 500 / 508 batches loss: 0.2642122805118561\n",
      "training epoch 17: avg epoch loss: 0.22784194350242615\n",
      "saving best model at epoch 17\n",
      "training epoch 18: 0 / 508 batches loss: 0.22801706194877625\n",
      "training epoch 18: 250 / 508 batches loss: 0.20750801265239716\n",
      "training epoch 18: 500 / 508 batches loss: 0.26298654079437256\n",
      "training epoch 18: avg epoch loss: 0.2269805371761322\n",
      "saving best model at epoch 18\n",
      "training epoch 19: 0 / 508 batches loss: 0.22619357705116272\n",
      "training epoch 19: 250 / 508 batches loss: 0.20696111023426056\n",
      "training epoch 19: 500 / 508 batches loss: 0.26244720816612244\n",
      "training epoch 19: avg epoch loss: 0.22619526088237762\n",
      "saving best model at epoch 19\n",
      "training epoch 20: 0 / 508 batches loss: 0.22501251101493835\n",
      "training epoch 20: 250 / 508 batches loss: 0.20582595467567444\n",
      "training epoch 20: 500 / 508 batches loss: 0.2614985406398773\n",
      "training epoch 20: avg epoch loss: 0.22542613744735718\n",
      "saving best model at epoch 20\n",
      "training epoch 21: 0 / 508 batches loss: 0.22464929521083832\n",
      "training epoch 21: 250 / 508 batches loss: 0.20577096939086914\n",
      "training epoch 21: 500 / 508 batches loss: 0.2608081102371216\n",
      "training epoch 21: avg epoch loss: 0.2246839553117752\n",
      "saving best model at epoch 21\n",
      "training epoch 22: 0 / 508 batches loss: 0.22425006330013275\n",
      "training epoch 22: 250 / 508 batches loss: 0.20539094507694244\n",
      "training epoch 22: 500 / 508 batches loss: 0.26061102747917175\n",
      "training epoch 22: avg epoch loss: 0.22400832176208496\n",
      "saving best model at epoch 22\n",
      "training epoch 23: 0 / 508 batches loss: 0.2227444350719452\n",
      "training epoch 23: 250 / 508 batches loss: 0.2046455591917038\n",
      "training epoch 23: 500 / 508 batches loss: 0.260669082403183\n",
      "training epoch 23: avg epoch loss: 0.22346104681491852\n",
      "saving best model at epoch 23\n",
      "training epoch 24: 0 / 508 batches loss: 0.22175364196300507\n",
      "training epoch 24: 250 / 508 batches loss: 0.20456604659557343\n",
      "training epoch 24: 500 / 508 batches loss: 0.2599028944969177\n",
      "training epoch 24: avg epoch loss: 0.22291095554828644\n",
      "saving best model at epoch 24\n",
      "training epoch 25: 0 / 508 batches loss: 0.22176377475261688\n",
      "training epoch 25: 250 / 508 batches loss: 0.20389597117900848\n",
      "training epoch 25: 500 / 508 batches loss: 0.2598859369754791\n",
      "training epoch 25: avg epoch loss: 0.2223910391330719\n",
      "saving best model at epoch 25\n",
      "training epoch 26: 0 / 508 batches loss: 0.2205580323934555\n",
      "training epoch 26: 250 / 508 batches loss: 0.2034883201122284\n",
      "training epoch 26: 500 / 508 batches loss: 0.2598801851272583\n",
      "training epoch 26: avg epoch loss: 0.22190196812152863\n",
      "saving best model at epoch 26\n",
      "training epoch 27: 0 / 508 batches loss: 0.22059178352355957\n",
      "training epoch 27: 250 / 508 batches loss: 0.20353847742080688\n",
      "training epoch 27: 500 / 508 batches loss: 0.25915274024009705\n",
      "training epoch 27: avg epoch loss: 0.22143705189228058\n",
      "saving best model at epoch 27\n",
      "training epoch 28: 0 / 508 batches loss: 0.21884295344352722\n",
      "training epoch 28: 250 / 508 batches loss: 0.20283502340316772\n",
      "training epoch 28: 500 / 508 batches loss: 0.2592256963253021\n",
      "training epoch 28: avg epoch loss: 0.2209148406982422\n",
      "saving best model at epoch 28\n",
      "training epoch 29: 0 / 508 batches loss: 0.2179589569568634\n",
      "training epoch 29: 250 / 508 batches loss: 0.20298314094543457\n",
      "training epoch 29: 500 / 508 batches loss: 0.25875934958457947\n",
      "training epoch 29: avg epoch loss: 0.22048312425613403\n",
      "saving best model at epoch 29\n",
      "training epoch 30: 0 / 508 batches loss: 0.21821482479572296\n",
      "training epoch 30: 250 / 508 batches loss: 0.20259954035282135\n",
      "training epoch 30: 500 / 508 batches loss: 0.2583845555782318\n",
      "training epoch 30: avg epoch loss: 0.22005963325500488\n",
      "saving best model at epoch 30\n",
      "training epoch 31: 0 / 508 batches loss: 0.2180487960577011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 31: 250 / 508 batches loss: 0.20196767151355743\n",
      "training epoch 31: 500 / 508 batches loss: 0.25800472497940063\n",
      "training epoch 31: avg epoch loss: 0.2197038233280182\n",
      "saving best model at epoch 31\n",
      "training epoch 32: 0 / 508 batches loss: 0.21762454509735107\n",
      "training epoch 32: 250 / 508 batches loss: 0.20212188363075256\n",
      "training epoch 32: 500 / 508 batches loss: 0.2576419711112976\n",
      "training epoch 32: avg epoch loss: 0.2192840576171875\n",
      "saving best model at epoch 32\n",
      "training epoch 33: 0 / 508 batches loss: 0.2179744392633438\n",
      "training epoch 33: 250 / 508 batches loss: 0.20120635628700256\n",
      "training epoch 33: 500 / 508 batches loss: 0.2578238844871521\n",
      "training epoch 33: avg epoch loss: 0.21896842122077942\n",
      "saving best model at epoch 33\n",
      "training epoch 34: 0 / 508 batches loss: 0.21740181744098663\n",
      "training epoch 34: 250 / 508 batches loss: 0.20090530812740326\n",
      "training epoch 34: 500 / 508 batches loss: 0.2572774887084961\n",
      "training epoch 34: avg epoch loss: 0.21857377886772156\n",
      "saving best model at epoch 34\n",
      "training epoch 35: 0 / 508 batches loss: 0.21638278663158417\n",
      "training epoch 35: 250 / 508 batches loss: 0.20090314745903015\n",
      "training epoch 35: 500 / 508 batches loss: 0.2568514347076416\n",
      "training epoch 35: avg epoch loss: 0.2182074785232544\n",
      "saving best model at epoch 35\n",
      "training epoch 36: 0 / 508 batches loss: 0.21604099869728088\n",
      "training epoch 36: 250 / 508 batches loss: 0.20036524534225464\n",
      "training epoch 36: 500 / 508 batches loss: 0.25722819566726685\n",
      "training epoch 36: avg epoch loss: 0.21790450811386108\n",
      "saving best model at epoch 36\n",
      "training epoch 37: 0 / 508 batches loss: 0.2156544029712677\n",
      "training epoch 37: 250 / 508 batches loss: 0.1998886615037918\n",
      "training epoch 37: 500 / 508 batches loss: 0.25749096274375916\n",
      "training epoch 37: avg epoch loss: 0.21762342751026154\n",
      "saving best model at epoch 37\n",
      "training epoch 38: 0 / 508 batches loss: 0.21538864076137543\n",
      "training epoch 38: 250 / 508 batches loss: 0.199271559715271\n",
      "training epoch 38: 500 / 508 batches loss: 0.25718751549720764\n",
      "training epoch 38: avg epoch loss: 0.21726666390895844\n",
      "saving best model at epoch 38\n",
      "training epoch 39: 0 / 508 batches loss: 0.21551118791103363\n",
      "training epoch 39: 250 / 508 batches loss: 0.19958563148975372\n",
      "training epoch 39: 500 / 508 batches loss: 0.256661981344223\n",
      "training epoch 39: avg epoch loss: 0.21697022020816803\n",
      "saving best model at epoch 39\n",
      "training epoch 40: 0 / 508 batches loss: 0.21533457934856415\n",
      "training epoch 40: 250 / 508 batches loss: 0.19925887882709503\n",
      "training epoch 40: 500 / 508 batches loss: 0.256563276052475\n",
      "training epoch 40: avg epoch loss: 0.21676219999790192\n",
      "saving best model at epoch 40\n",
      "training epoch 41: 0 / 508 batches loss: 0.21471045911312103\n",
      "training epoch 41: 250 / 508 batches loss: 0.19935771822929382\n",
      "training epoch 41: 500 / 508 batches loss: 0.2559485137462616\n",
      "training epoch 41: avg epoch loss: 0.21650126576423645\n",
      "saving best model at epoch 41\n",
      "training epoch 42: 0 / 508 batches loss: 0.2140335738658905\n",
      "training epoch 42: 250 / 508 batches loss: 0.19892767071723938\n",
      "training epoch 42: 500 / 508 batches loss: 0.2563267946243286\n",
      "training epoch 42: avg epoch loss: 0.2161855250597\n",
      "saving best model at epoch 42\n",
      "training epoch 43: 0 / 508 batches loss: 0.21500664949417114\n",
      "training epoch 43: 250 / 508 batches loss: 0.199093759059906\n",
      "training epoch 43: 500 / 508 batches loss: 0.2559341490268707\n",
      "training epoch 43: avg epoch loss: 0.21602179110050201\n",
      "saving best model at epoch 43\n",
      "training epoch 44: 0 / 508 batches loss: 0.21436633169651031\n",
      "training epoch 44: 250 / 508 batches loss: 0.1984257698059082\n",
      "training epoch 44: 500 / 508 batches loss: 0.2553134262561798\n",
      "training epoch 44: avg epoch loss: 0.21574442088603973\n",
      "saving best model at epoch 44\n",
      "training epoch 45: 0 / 508 batches loss: 0.21416643261909485\n",
      "training epoch 45: 250 / 508 batches loss: 0.1984521448612213\n",
      "training epoch 45: 500 / 508 batches loss: 0.25563549995422363\n",
      "training epoch 45: avg epoch loss: 0.2155396044254303\n",
      "saving best model at epoch 45\n",
      "training epoch 46: 0 / 508 batches loss: 0.2136354297399521\n",
      "training epoch 46: 250 / 508 batches loss: 0.19841037690639496\n",
      "training epoch 46: 500 / 508 batches loss: 0.25492823123931885\n",
      "training epoch 46: avg epoch loss: 0.215334951877594\n",
      "saving best model at epoch 46\n",
      "training epoch 47: 0 / 508 batches loss: 0.2137579321861267\n",
      "training epoch 47: 250 / 508 batches loss: 0.19818507134914398\n",
      "training epoch 47: 500 / 508 batches loss: 0.2553659677505493\n",
      "training epoch 47: avg epoch loss: 0.21505212783813477\n",
      "saving best model at epoch 47\n",
      "training epoch 48: 0 / 508 batches loss: 0.21332396566867828\n",
      "training epoch 48: 250 / 508 batches loss: 0.19829142093658447\n",
      "training epoch 48: 500 / 508 batches loss: 0.25438711047172546\n",
      "training epoch 48: avg epoch loss: 0.21487297117710114\n",
      "saving best model at epoch 48\n",
      "training epoch 49: 0 / 508 batches loss: 0.21399638056755066\n",
      "training epoch 49: 250 / 508 batches loss: 0.19797782599925995\n",
      "training epoch 49: 500 / 508 batches loss: 0.25450044870376587\n",
      "training epoch 49: avg epoch loss: 0.2146233171224594\n",
      "saving best model at epoch 49\n"
     ]
    }
   ],
   "source": [
    "# running with relu activation with no softmax (modifications made in models.py)\n",
    "num_epochs = 50\n",
    "torch.manual_seed(32)\n",
    "epoch_loss = torch.zeros(num_epochs)\n",
    "num_batches = len(training_input)\n",
    "\n",
    "model_nn = NN(EMBEDDING_DIM * max_length, EMBEDDING_DIM)\n",
    "learning_rate = 0.01    \n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=learning_rate)\n",
    "best_model = 999\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    model_nn.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(training_input):\n",
    "        current_batch_size = target.size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model_nn(data)\n",
    "        mask = Variable(torch.ones(current_batch_size), requires_grad=False)\n",
    "        loss = F.cosine_embedding_loss(output, target, mask) # the model uses cosine embedding loss\n",
    "        total_epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 250 == 0:\n",
    "            print('training epoch {}: {} / {} batches loss: {}'.format(epoch, batch_idx, num_batches, loss))\n",
    "\n",
    "    epoch_loss[epoch] = total_epoch_loss / num_batches\n",
    "    print('training epoch {}: avg epoch loss: {}'.format(epoch, epoch_loss[epoch]))\n",
    "    if epoch_loss[epoch] < best_model:\n",
    "        best_model = epoch_loss[epoch]\n",
    "        print('saving best model at epoch {}'.format(epoch))\n",
    "        torch.save(model_nn, 'nn.nosoftmax.best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2903, 0.2678, 0.2598, 0.2544, 0.2504, 0.2471, 0.2444, 0.2421, 0.2400,\n",
       "        0.2382, 0.2364, 0.2349, 0.2334, 0.2321, 0.2309, 0.2298, 0.2288, 0.2278,\n",
       "        0.2270, 0.2262, 0.2254, 0.2247, 0.2240, 0.2235, 0.2229, 0.2224, 0.2219,\n",
       "        0.2214, 0.2209, 0.2205, 0.2201, 0.2197, 0.2193, 0.2190, 0.2186, 0.2182,\n",
       "        0.2179, 0.2176, 0.2173, 0.2170, 0.2168, 0.2165, 0.2162, 0.2160, 0.2157,\n",
       "        0.2155, 0.2153, 0.2151, 0.2149, 0.2146], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
